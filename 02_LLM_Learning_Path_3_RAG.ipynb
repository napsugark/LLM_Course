{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/napsugark/LLM_Course/blob/main/02_LLM_Learning_Path_3_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Retrieval Augmented Generation**"
      ],
      "metadata": {
        "id": "rV1AILhzELgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to old version: https://colab.research.google.com/drive/1JWVqwwrlUQrABqz4b5r4IY85kz1itvlr#scrollTo=_OnPDWm577oq"
      ],
      "metadata": {
        "id": "J2KYmCV4SnWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concepts to know\n"
      ],
      "metadata": {
        "id": "e0wrBqMsEJcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this module you should have an understanding of the following concepts:"
      ],
      "metadata": {
        "id": "ndpYtkoy0DlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Methods of LLM adaptation (Fine Tuning, RAG, In-context learning)\n",
        "- Components of a RAG system\n",
        "- Context\n",
        "- Vector Database\n",
        "- Retriever Systems (BM25, vectors, SQL, Graph, Ensemble)\n",
        "- Metadata filtering\n",
        "- Chunking\n",
        "- Query Rewriting (e.g. HyDE)\n",
        "\n"
      ],
      "metadata": {
        "id": "_OnPDWm577oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Materials\n"
      ],
      "metadata": {
        "id": "7AIipnHRDvgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mandatory:\n",
        "LLM Adaptation Overview:\n",
        "- https://ai.meta.com/blog/adapting-large-language-models-llms/\n",
        "- https://ai.meta.com/blog/when-to-fine-tune-llms-vs-other-techniques/\n",
        "\n",
        "Retrieval Augmented Generation:\n",
        "- https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag\n",
        "- Course - RAG from Scratch (PART 1-4): https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x\n",
        "\n",
        "Retrieval:\n",
        "- https://python.langchain.com/docs/concepts/retrieval/\n",
        "\n",
        "\n",
        "\n",
        "### Additional Material:\n",
        "- Langchain RAG Tutorial : https://python.langchain.com/docs/tutorials/rag/\n",
        "- Microsoft GenAI for Beginners on RAG: https://learn.microsoft.com/en-us/shows/generative-ai-for-beginners/retrieval-augmented-generation-rag-and-vector-databases-generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst\n",
        "- Research Survey Summary: https://www.promptingguide.ai/research/rag\n",
        "- Visualization of Chunking: https://chunkviz.up.railway.app/\n",
        "- Retrieval Strategies:https://medium.com/@vinayak.sengupta/exploring-the-core-of-augmented-intelligence-advancing-the-power-of-retrievers-in-rag-frameworks-3ef9fe273764\n",
        "\n",
        "Vector Stores:\n",
        "- https://python.langchain.com/docs/concepts/vectorstores/\n",
        "- https://www.datacamp.com/blog/the-top-5-vector-databases\n",
        "\n",
        "\n",
        "Chunking:\n",
        "- https://www.pinecone.io/learn/chunking-strategies/\n",
        "\n",
        "Document Parsing:\n",
        "- https://www.youtube.com/watch?v=9lBTS5dM27c&ab_channel=DaveEbbelaar\n",
        "\n",
        "LoRa:\n",
        "- https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation\n",
        "\n",
        "RAFT:\n",
        "- https://techcommunity.microsoft.com/blog/aiplatformblog/raft-a-new-way-to-teach-llms-to-be-better-at-rag/4084674\n",
        "\n",
        "Adaptation:\n",
        "- https://learn.microsoft.com/en-us/azure/developer/ai/augment-llm-rag-fine-tuning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UnKyGLJ_8IKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding"
      ],
      "metadata": {
        "id": "THjfc6BMKHL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the following to run the coding examples below"
      ],
      "metadata": {
        "id": "3WAp1mHtWKva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community rank_bm25 openai langchain_openai langchain_chroma wikipedia"
      ],
      "metadata": {
        "id": "I1RdcjPDLnJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Build a simple vector DB RAG from scratch\n"
      ],
      "metadata": {
        "id": "DJMnqKVXEXPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin with, we are going to build a vector database on a very simple dataset containing random facts on cats and dogs."
      ],
      "metadata": {
        "id": "g1vb2MOCSA5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = [\n",
        "    \"Cats can rotate their ears 180 degrees to better locate sounds.\",\n",
        "    \"Dogs have a sense of smell that's 10,000 to 100,000 times more sensitive than humans.\",\n",
        "    \"Cats spend approximately 70% of their lives sleeping.\",\n",
        "    \"The Basenji dog breed is unique because it doesn't bark; it yodels instead.\",\n",
        "    \"Domestic cats are descendants of African wildcats, which were first domesticated about 9,000 years ago.\",\n",
        "    \"Dogs are known to dream, and puppies and older dogs tend to dream more frequently.\",\n",
        "    \"A group of kittens is called a 'kindle,' while a group of adult cats is called a 'clowder.'\",\n",
        "    \"Dalmatians are born completely white and develop their spots as they grow.\",\n",
        "    \"Cats have a specialized collarbone that allows them to always land on their feet when falling.\",\n",
        "    \"The Labrador Retriever has been the most popular dog breed in the United States for decades.\",\n",
        "    \"Cats can rotate their ears 180 degrees which makes them great hunters.\"\n",
        "]"
      ],
      "metadata": {
        "id": "az80Jsdu_tsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the Azure OpenAI client\n",
        "from openai import AzureOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint = userdata.get('AZURE_OPENAI_ENDPOINT'),\n",
        "    api_key=userdata.get('AZURE_OPENAI_API_KEY'),\n",
        "    api_version=\"2024-06-01\" )"
      ],
      "metadata": {
        "id": "y4V5NFr3_tMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For creating a vector database, we need an embedding that transforms each piece of text into an embedding, e.g. a numerical representation."
      ],
      "metadata": {
        "id": "cOEIb_oKSWbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL = \"text-embedding-ada-002\""
      ],
      "metadata": {
        "id": "lFyUqvLGSXNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simple_vector_database will be a list of tuples containing the original text and the embedding of the text."
      ],
      "metadata": {
        "id": "0v2GFDpYStxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector DB\n",
        "simple_vector_db = []\n",
        "\n",
        "def get_embedding(chunk):\n",
        "    response = client.embeddings.create(\n",
        "        input=[chunk],\n",
        "        model=EMBEDDING_MODEL\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def add_chunk_to_database(chunk):\n",
        "    embedding = get_embedding(chunk)\n",
        "    simple_vector_db.append((chunk, embedding))"
      ],
      "metadata": {
        "id": "B_109crxATOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate the database\n",
        "for i, chunk in enumerate(dataset):\n",
        "    add_chunk_to_database(chunk)\n",
        "    print(f'Added chunk {i+1}/{len(dataset)} to the database')"
      ],
      "metadata": {
        "id": "RKk32JLMAqjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding created has the dimension of 1536, i.e. 1536 numbers representing the original text."
      ],
      "metadata": {
        "id": "iK0cifxDTiAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(simple_vector_db[0][1])"
      ],
      "metadata": {
        "id": "fheAbdImTXEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a vector database, we need to have a way to retrieve from it.\n",
        "\n",
        "We will use the embeddings to retrieve texts that are numerically similar to an input query. For this we need a function that calculates the cosine similarity between two texts. The retrieve function then calculates the embedding of the input query and the embeddings of the texts in the vector db, returning the top_n most similar texts from the vector database."
      ],
      "metadata": {
        "id": "ZDvzVMBLTrwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define similarity Retriever\n",
        "def cosine_similarity(a, b):\n",
        "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
        "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
        "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "def retrieve(query, top_n=3):\n",
        "    query_embedding = get_embedding(query)\n",
        "    similarities = []\n",
        "    for chunk, embedding in simple_vector_db:\n",
        "        similarity = cosine_similarity(query_embedding, embedding)\n",
        "        similarities.append((chunk, similarity))\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_n]"
      ],
      "metadata": {
        "id": "sJi4dzVjAKXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try our vector db retriever:"
      ],
      "metadata": {
        "id": "r0sznoSNUhgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve('How far can cats rotate their ears?')\n",
        "#"
      ],
      "metadata": {
        "id": "mhu2slcUUDrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step to a RAG chatbot is to pass the retrieved information along with the original query to a LLM that generates an answer based on the information it receives."
      ],
      "metadata": {
        "id": "i0xlWlxIU-aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot\n",
        "\n",
        "GENERATION_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "input_query = 'How far can cats rotate their ears? '\n",
        "\n",
        "retrieved_knowledge = retrieve(input_query)\n",
        "\n",
        "print('Retrieved knowledge:')\n",
        "for chunk, similarity in retrieved_knowledge:\n",
        "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
        "\n",
        "instruction_prompt = f\"\"\"You are a helpful chatbot.\n",
        "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
        "{chr(10).join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=GENERATION_MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": instruction_prompt},\n",
        "        {\"role\": \"user\", \"content\": input_query},\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print the chatbot response\n",
        "print('Chatbot response:')\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "fdccfCmLWRZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment:\n",
        "Try out what happens, if you ask the chatbot something that is not in the cats & dogs datase, e.g. how to make a full English breakfast."
      ],
      "metadata": {
        "id": "SRGIXOfvWn7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIySzpBuzyEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Populate a ChromaDB vector database"
      ],
      "metadata": {
        "id": "gP7WqOMXDajJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of just having a list of tuples, lets now use the cats&dogs dataset to pupulate a vector db with Chroma DB."
      ],
      "metadata": {
        "id": "_yB0HkmMV7ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_chroma import Chroma"
      ],
      "metadata": {
        "id": "ij0cu51XKDgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding functions and ChromaDB\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    model=EMBEDDING_MODEL,\n",
        "    api_key=userdata.get('AZURE_OPENAI_API_KEY'),\n",
        "    api_version=\"2024-06-01\",\n",
        "    azure_endpoint=userdata.get('AZURE_OPENAI_ENDPOINT')\n",
        ")\n",
        "vector_db = Chroma(\n",
        "    collection_name=\"cats-and-dogs\",\n",
        "    embedding_function=embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "Vtd7Bd5DKGhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add texts to the vectorDB\n",
        "vector_db.add_texts(dataset)"
      ],
      "metadata": {
        "id": "TV-ITFIfDhxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_db.similarity_search(\n",
        "    \"How far can cats rotate their ears\",\n",
        "    k=2,\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content}\")"
      ],
      "metadata": {
        "id": "53szD0o8KJ3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment:\n",
        "\n",
        "Try out what happens if you perform a similarity search on the input queries \"How far can dogs rotate their ears?\". Why do you think that is? Could this be a problem?"
      ],
      "metadata": {
        "id": "b2TpCFHfW9R5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "odUpQyIQYzJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Metadata filtering"
      ],
      "metadata": {
        "id": "UE9zqLZGEjDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to solve the problem observed above is to introduce metadata into the vector DB. In this case, it would be a label indicating be whether the text relates to cats or dogs."
      ],
      "metadata": {
        "id": "SJUoYRQnXcNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataset\n",
        "vector_db_metadata = Chroma(\n",
        "    collection_name=\"cats-and-dogs-metadata\",\n",
        "    embedding_function=embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "0yCCuKwwNpiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = [{\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"},]"
      ],
      "metadata": {
        "id": "yn2V4wcdN-jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can pass metadata objects as dictionaries when you add data to the vector DB."
      ],
      "metadata": {
        "id": "EuKCDtQdX0Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db_metadata.from_texts(\n",
        "    dataset,\n",
        "    collection_name=\"cats-and-dogs-metadata\",\n",
        "    metadatas = metadata,\n",
        "    embedding = embeddings\n",
        "    )"
      ],
      "metadata": {
        "id": "ck8yhMYWN0E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a vector DB with metadata you can perform similarity search with previous filtering, i.e. for a question concerning docs you only look at entries that have that attribute."
      ],
      "metadata": {
        "id": "lbOGr-0HX9-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_db_metadata.similarity_search(\n",
        "    \"How far can dogs rotate their ears?\",\n",
        "    k=2,\n",
        "    filter={\"Animal\": \"Dog\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content}\")"
      ],
      "metadata": {
        "id": "2KA667KQNnAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: MMR retrieval"
      ],
      "metadata": {
        "id": "hLGNPVIjD4l5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max mariginal relevance is a different retrieval method. Execute the code cell below and obeserve how the result changes in comparison to the standard similiarity search. Why do you think that is?"
      ],
      "metadata": {
        "id": "Gg9HTcyuY30U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db.max_marginal_relevance_search(\n",
        "    query = \"How far can cats rotate their ears\",\n",
        "    k = 2,\n",
        "    fetch_k = 4)"
      ],
      "metadata": {
        "id": "olsEDRyxMqb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Similarity Threshold"
      ],
      "metadata": {
        "id": "GZ1agLHjEbyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also set a similarity threshold, meaning that results are only retrieved from the db if their similiarity is above a defined threshold."
      ],
      "metadata": {
        "id": "WqS3o2mGeVr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.4}\n",
        ")"
      ],
      "metadata": {
        "id": "REVP912PcV33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"How do you call a group of kittens?\")"
      ],
      "metadata": {
        "id": "-nQ2u3YLcodd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8}\n",
        ")"
      ],
      "metadata": {
        "id": "29-NuQtXd4Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"How do you call a group of kittens?\")"
      ],
      "metadata": {
        "id": "aKNuh3Icd7tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"How do I make a group of pancakes?\")"
      ],
      "metadata": {
        "id": "8K-4EhbFeJul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Ensemble Retriever"
      ],
      "metadata": {
        "id": "F7m6RnfVEfAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You do not have to use a single retriever, but can combine different retrieval methods with an EnsembleRetriever."
      ],
      "metadata": {
        "id": "MuyvjBlnfGJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
      ],
      "metadata": {
        "id": "QPhulLVdQE5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_retriever= vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "\n",
        "keyword_retriever = BM25Retriever.from_texts(dataset)\n",
        "keyword_retriever.k =  2\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[semantic_retriever,\n",
        "                                                   keyword_retriever],\n",
        "                                       weights=[0.5, 0.5])"
      ],
      "metadata": {
        "id": "EYXVTxLPOT9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_retriever.invoke(\"How do you call a group of kittens?\")"
      ],
      "metadata": {
        "id": "WAVUULKUe4Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_retriever.invoke(\"How do you call a group of kittens?\")"
      ],
      "metadata": {
        "id": "pF0_SevEe_au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"How do you call a group of kittens?\")"
      ],
      "metadata": {
        "id": "n9tR4cWVexQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Build a RAG chatbot with langchain"
      ],
      "metadata": {
        "id": "Bm1WldMLC2eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's rebuild the simple, from scratch cats and dogs retriever from above using langchain."
      ],
      "metadata": {
        "id": "ZlkOmr0nZQTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "xHnBzqTQaHRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model you want to use\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    api_key=userdata.get('AZURE_OPENAI_API_KEY'),\n",
        "    api_version=\"2023-06-01-preview\",\n",
        "    azure_endpoint=userdata.get('AZURE_OPENAI_ENDPOINT')\n",
        ")"
      ],
      "metadata": {
        "id": "DQUJyubqJpXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "eg6e9oZFI10E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt for the generation\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know..\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "Zm4bRBILZ5qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will define the chatbot as a chain, that first uses the retriever and a formatting function that prepares the output in a for the LLM, than populates the prompt with the results, calls the LLM and returns the response."
      ],
      "metadata": {
        "id": "cUjHXt3tZykK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ],
      "metadata": {
        "id": "qHOgR6ywCtRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, invoking the chain will give you the response of the chatbot.\n"
      ],
      "metadata": {
        "id": "oUXJmBHzaU3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(\"How far can cats rotate their ears?\")"
      ],
      "metadata": {
        "id": "531LaRlqaTlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Add website text as corpus to the RAG chabot"
      ],
      "metadata": {
        "id": "O3lvdZgEC7aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we used just a few sentences on cats and dogs as context. Vector databases will be typically containing many more and much larger documents. So, let's build a vector db that contains wikipedia articles on some movies. Langchain offeres a retriever class to get articles from Wikipedia."
      ],
      "metadata": {
        "id": "lvLn6hVHcIe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "\n",
        "retriever = WikipediaRetriever(\n",
        "    top_k_results = 1,\n",
        "    lang = 'en',\n",
        ")"
      ],
      "metadata": {
        "id": "gpuylVUQiS0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_docs =[]\n",
        "movies = [\"Inception\", \"Django Unchained\", \"Shutter Island\", \"The Dark Knight\"]"
      ],
      "metadata": {
        "id": "xyWLVOvUipMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for movie in movies:\n",
        "  movie_docs += retriever.invoke(movie)"
      ],
      "metadata": {
        "id": "T52Ca5ldgJZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The retriever returns a document object, which is used by langchain to process and populate databases. It contains metadata (in this case title, source link and a summary) and the actual text retrieved as page content."
      ],
      "metadata": {
        "id": "1_2Jluj7iAnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_docs[0]"
      ],
      "metadata": {
        "id": "K1qu6-Roj1ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Longer documents can be split up into smaller parts with the RecursiveCharacterTextSplitter."
      ],
      "metadata": {
        "id": "8kbF3eNGijzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
        "\n",
        "movie_docs_split = text_splitter.split_documents(movie_docs)\n"
      ],
      "metadata": {
        "id": "7zQ9kzDcJWTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for movie in movie_docs:\n",
        "  print(movie.metadata['title'])"
      ],
      "metadata": {
        "id": "ipNE2pyGi9PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(movie_docs)"
      ],
      "metadata": {
        "id": "8iLG1wJTBiZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(movie_docs_split)"
      ],
      "metadata": {
        "id": "xPiSpJrjjYoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets create a vector db from the split documents and define a movie RAG chain."
      ],
      "metadata": {
        "id": "-laYlK5ClOD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_vector_db = Chroma.from_documents(documents=movie_docs_split, embedding=embeddings, persist_directory=\"persist\")"
      ],
      "metadata": {
        "id": "d_3zAqZ7JePr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_retriever = movie_vector_db.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "_wcXqGAaBoIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt for the generation\n",
        "movie_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an assistant for question on famous movies. Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know. Do not answer any question that are not related to movies.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "JaiDwOLLkvBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_retriever.invoke(\"Who plays in Inception?\")"
      ],
      "metadata": {
        "id": "-uk1TJ4RkB9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "movie_rag_chain = (\n",
        "    {\"context\": movie_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | movie_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ],
      "metadata": {
        "id": "1f8w5RNVk8pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_rag_chain.invoke('Who plays in Inception?')"
      ],
      "metadata": {
        "id": "LNTLJDQ7lDHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: RAG chatbot with memory in chainlit"
      ],
      "metadata": {
        "id": "Vh3SnpFxp-tT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take the simple chatbot implementation in chainlit and turn it into a RAG chatbot with a vector DB on movies.\n"
      ],
      "metadata": {
        "id": "CDG4su_aWZe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Make a venv with python -m .venv venv\n",
        "- Activate .venv with .venv\\Scripts\\activate\n"
      ],
      "metadata": {
        "id": "o3rPqWHw5QES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Make a .env file"
      ],
      "metadata": {
        "id": "ADgv9AJHBxV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AZURE_OPENAI_ENDPOINT=<insert endpoint>\n",
        "AZURE_OPENAI_API_KEY=<insert key>\n",
        "QDRANT_ENDPOINT=<insert Qdrant Endpoint>\n",
        "QDRANT_API_KEY=<insert API Key>"
      ],
      "metadata": {
        "id": "FBggOi1bByTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Make a requirements.txt file"
      ],
      "metadata": {
        "id": "FwYTzO3pBsTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "langchain_openai\n",
        "qdrant-client\n",
        "langchain_qdrant\n",
        "langchain_core\n",
        "chainlit\n",
        "openai\n",
        "wikipedia\n",
        "langchain_community\n",
        "python-dotenv"
      ],
      "metadata": {
        "id": "WcLm7PjSB-r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Create the following create_db.py file and execute with python create_db.py to create a qdrant collection\n"
      ],
      "metadata": {
        "id": "qEJmXfsr5eTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import VectorParams, Distance\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "    api_version=\"2024-06-01\",\n",
        "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        ")\n",
        "\n",
        "retriever = WikipediaRetriever(\n",
        "    top_k_results = 1,\n",
        "    lang = 'en',\n",
        ")\n",
        "qdrant_client = QdrantClient(\n",
        "    url=os.getenv('QDRANT_ENDPOINT'),\n",
        "    api_key=os.getenv('QDRANT_API_KEY')\n",
        ")\n",
        "if not qdrant_client.collection_exists(collection_name=\"local_movie_db\"):\n",
        "    qdrant_client.create_collection(\n",
        "        collection_name=\"local_movie_db\",\n",
        "        vectors_config=VectorParams(\n",
        "            size=1536,\n",
        "            distance=Distance.COSINE\n",
        "        )\n",
        "    )\n",
        "qdrant_db = QdrantVectorStore(\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"local_movie_db\",\n",
        "    embedding=embeddings\n",
        ")\n",
        "movie_docs = []\n",
        "movies = [\"Inception\", \"The Return of the King\", \"Shutter Island\", \"The Dark Knight\"]\n",
        "\n",
        "for movie in movies:\n",
        "  movie_docs += retriever.invoke(movie)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
        "\n",
        "movie_docs_split = text_splitter.split_documents(movie_docs)\n",
        "\n",
        "movie_vector_db = qdrant_db.add_documents(documents=movie_docs_split)\n"
      ],
      "metadata": {
        "id": "Yx2mE8c05q7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create the following rag_chatbot.py file and execute with chainlit run rag_chatbot.py -w"
      ],
      "metadata": {
        "id": "quBTyL615t-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "import chainlit as cl\n",
        "import os\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from chainlit.input_widget import Select, Slider\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start_chat():\n",
        "    await cl.ChatSettings([\n",
        "        Select(\n",
        "            id=\"language\",\n",
        "            values=[\"English\", \"Romanian\"],\n",
        "            label=\"Select your preferred language\",\n",
        "            initial_value=\"English\"\n",
        "        ),\n",
        "        Slider(\n",
        "            id=\"Temperature\",\n",
        "            label=\"Temperature\",\n",
        "            initial=0,\n",
        "            min=0,\n",
        "            max=1,\n",
        "            step=0.1\n",
        "        )\n",
        "    ]\n",
        "    ).send()\n",
        "    cl.user_session.set(\"chat_history\", [])\n",
        "    cl.user_session.set(\"client\", AzureOpenAI(\n",
        "        azure_endpoint=os.environ.get('AZURE_OPENAI_ENDPOINT'),\n",
        "        api_key=os.environ.get('AZURE_OPENAI_API_KEY'),\n",
        "        api_version=\"2024-06-01\"))\n",
        "    cl.user_session.set(\"embedding_model\", AzureOpenAIEmbeddings(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "        api_version=\"2024-06-01\",\n",
        "        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        "    ))\n",
        "    cl.user_session.set(\"qdrant_client\", QdrantClient(\n",
        "            url=os.getenv('QDRANT_ENDPOINT'),\n",
        "            api_key=os.getenv('QDRANT_API_KEY')\n",
        "    ))\n",
        "    cl.user_session.set(\"retriever\", QdrantVectorStore(\n",
        "        collection_name='local_movie_db',\n",
        "        embedding=cl.user_session.get(\"embedding_model\"),\n",
        "        client=cl.user_session.get(\"qdrant_client\")))\n",
        "\n",
        "\n",
        "def get_system_prompt(language):\n",
        "    return f\"\"\"\n",
        "    You are a helpful assistant for question on famous movies.\n",
        "    You will formulate all its answers in {language}.\n",
        "    Base you answer only on pieces of information received as context below.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Do not answer any question that are not related to movies.\"\"\"\n",
        "\n",
        "\n",
        "@cl.on_settings_update\n",
        "async def setup_agent(settings):\n",
        "    cl.user_session.set(\"language\", settings[\"language\"])\n",
        "    cl.user_session.set(\"temperature\", settings[\"Temperature\"])\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "@cl.on_message\n",
        "async def message_send(message: cl.Message):\n",
        "    language = cl.user_session.get(\"language\", \"English\")\n",
        "    temperature = cl.user_session.get(\"temperature\", 0)\n",
        "    chat_history = cl.user_session.get(\"chat_history\", [])\n",
        "    retriever = cl.user_session.get(\"retriever\")\n",
        "    client = cl.user_session.get(\"client\")\n",
        "    retrieved_docs = retriever.similarity_search(message.content, k=4)\n",
        "    context = format_docs(retrieved_docs)\n",
        "    system_prompt = get_system_prompt(language)\n",
        "    chat_history.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    chat_history.append({\"role\": \"user\", \"content\": f\"QUESTION: {message.content}\"})\n",
        "    chat_history.append({\"role\": \"system\", \"content\": f\"CONTEXT: {context}\"})\n",
        "    full_response = \"\"\n",
        "    source_elements = []\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=temperature,\n",
        "        stream=True,\n",
        "        messages=[\n",
        "            {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
        "            for m in chat_history\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    msg = cl.Message(content=\"\")\n",
        "    await msg.send()\n",
        "    for chunk in stream:\n",
        "        if not chunk.choices or not chunk.choices[0].delta:\n",
        "            continue\n",
        "\n",
        "        delta = chunk.choices[0].delta.content or \"\"\n",
        "        full_response += delta\n",
        "        await msg.stream_token(delta)\n",
        "    await msg.update()\n",
        "\n",
        "    source_elements.append(cl.Text(content=context, name=\"Context\", display=\"side\"))\n",
        "    msg.content += \"\\n\\nContext\"\n",
        "    msg.elements = source_elements\n",
        "    await msg.update()\n",
        "\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    cl.user_session.set(\"chat_history\", chat_history)"
      ],
      "metadata": {
        "id": "B55DEiTDqCmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment: Vector DB in Romanian and English"
      ],
      "metadata": {
        "id": "t5V-oMowU2C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve wikipedia articles on the movies in Romanian and English. Depending on the language selected, the retriever should only query the articles of the specified language."
      ],
      "metadata": {
        "id": "7SM_spct6gzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution TBD"
      ],
      "metadata": {
        "id": "0XSbZkoZ7bap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment: Standalone question"
      ],
      "metadata": {
        "id": "0NULNL02kiYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a reformulation step, that after the first conversation turn reformulates follow-up questions into standalone questions to be used for retrieval and generation. Use only user questions and llm responses for the reformulation step and only send the reformulated question plus retrieved context to the LLM."
      ],
      "metadata": {
        "id": "UQh9XGGG6qbb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhXeC4LaknX4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}