{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/napsugark/LLM_Course/blob/main/02_LLM_Learning_Path_3_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV1AILhzELgM"
      },
      "source": [
        "\n",
        "# **Retrieval Augmented Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2KYmCV4SnWy"
      },
      "source": [
        "Link to old version: https://colab.research.google.com/drive/1JWVqwwrlUQrABqz4b5r4IY85kz1itvlr#scrollTo=_OnPDWm577oq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0wrBqMsEJcC"
      },
      "source": [
        "# Concepts to know\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndpYtkoy0DlQ"
      },
      "source": [
        "At the end of this module you should have an understanding of the following concepts:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OnPDWm577oq"
      },
      "source": [
        "- Methods of LLM adaptation (Fine Tuning, RAG, In-context learning)\n",
        "- Components of a RAG system\n",
        "- Context\n",
        "- Vector Database\n",
        "- Retriever Systems (BM25, vectors, SQL, Graph, Ensemble)\n",
        "- Metadata filtering\n",
        "- Chunking\n",
        "- Query Rewriting (e.g. HyDE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AIipnHRDvgr"
      },
      "source": [
        "# Materials\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnKyGLJ_8IKQ"
      },
      "source": [
        "### Mandatory:\n",
        "LLM Adaptation Overview:\n",
        "- https://ai.meta.com/blog/adapting-large-language-models-llms/ - DONE\n",
        "- https://ai.meta.com/blog/when-to-fine-tune-llms-vs-other-techniques/ - DONE\n",
        "\n",
        "Retrieval Augmented Generation:\n",
        "- https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag - DONE\n",
        "- Course - RAG from Scratch (PART 1-4): https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x - DONE\n",
        "\n",
        "Retrieval:\n",
        "- https://python.langchain.com/docs/concepts/retrieval/ - DONE\n",
        "\n",
        "\n",
        "\n",
        "### Additional Material:\n",
        "- Langchain RAG Tutorial : https://python.langchain.com/docs/tutorials/rag/\n",
        "- Microsoft GenAI for Beginners on RAG: https://learn.microsoft.com/en-us/shows/generative-ai-for-beginners/retrieval-augmented-generation-rag-and-vector-databases-generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst\n",
        "- Research Survey Summary: https://www.promptingguide.ai/research/rag\n",
        "- Visualization of Chunking: https://chunkviz.up.railway.app/\n",
        "- Retrieval Strategies:https://medium.com/@vinayak.sengupta/exploring-the-core-of-augmented-intelligence-advancing-the-power-of-retrievers-in-rag-frameworks-3ef9fe273764\n",
        "\n",
        "Vector Stores:\n",
        "- https://python.langchain.com/docs/concepts/vectorstores/\n",
        "- https://www.datacamp.com/blog/the-top-5-vector-databases\n",
        "\n",
        "\n",
        "Chunking:\n",
        "- https://www.pinecone.io/learn/chunking-strategies/\n",
        "\n",
        "Document Parsing:\n",
        "- https://www.youtube.com/watch?v=9lBTS5dM27c&ab_channel=DaveEbbelaar\n",
        "\n",
        "LoRa:\n",
        "- https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation\n",
        "\n",
        "RAFT:\n",
        "- https://techcommunity.microsoft.com/blog/aiplatformblog/raft-a-new-way-to-teach-llms-to-be-better-at-rag/4084674\n",
        "\n",
        "Adaptation:\n",
        "- https://learn.microsoft.com/en-us/azure/developer/ai/augment-llm-rag-fine-tuning\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THjfc6BMKHL0"
      },
      "source": [
        "# Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WAp1mHtWKva"
      },
      "source": [
        "Install the following to run the coding examples below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I1RdcjPDLnJr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (0.3.29)\n",
            "Requirement already satisfied: rank_bm25 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (0.2.2)\n",
            "Requirement already satisfied: openai in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (1.104.2)\n",
            "Requirement already satisfied: langchain_openai in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (0.3.32)\n",
            "Requirement already satisfied: langchain_chroma in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (0.2.5)\n",
            "Requirement already satisfied: wikipedia in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (1.4.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (0.4.23)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_community) (2.3.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain_chroma) (1.0.20)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: build>=1.0.3 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (0.17.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (3.11.3)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (14.1.0)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from chromadb>=1.0.9->langchain_chroma) (4.25.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from requests<3,>=2.32.5->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from requests<3,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2025.9.1)\n",
            "Requirement already satisfied: colorama in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: pyproject_hooks in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb>=1.0.9->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.40.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.3.1)\n",
            "Requirement already satisfied: durationpy>=0.7 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.10)\n",
            "Requirement already satisfied: coloredlogs in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (6.32.0)\n",
            "Requirement already satisfied: sympy in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain_chroma) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.6.4)\n",
            "Requirement already satisfied: watchfiles>=0.13 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (2025.9.0)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (3.5.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\my_files\\learning\\llm_course\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.6.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_community rank_bm25 openai langchain_openai langchain_chroma wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJMnqKVXEXPF"
      },
      "source": [
        "## Example: Build a simple vector DB RAG from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1vb2MOCSA5S"
      },
      "source": [
        "To begin with, we are going to build a vector database on a very simple dataset containing random facts on cats and dogs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "az80Jsdu_tsx"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = [\n",
        "    \"Cats can rotate their ears 180 degrees to better locate sounds.\",\n",
        "    \"Dogs have a sense of smell that's 10,000 to 100,000 times more sensitive than humans.\",\n",
        "    \"Cats spend approximately 70% of their lives sleeping.\",\n",
        "    \"The Basenji dog breed is unique because it doesn't bark; it yodels instead.\",\n",
        "    \"Domestic cats are descendants of African wildcats, which were first domesticated about 9,000 years ago.\",\n",
        "    \"Dogs are known to dream, and puppies and older dogs tend to dream more frequently.\",\n",
        "    \"A group of kittens is called a 'kindle,' while a group of adult cats is called a 'clowder.'\",\n",
        "    \"Dalmatians are born completely white and develop their spots as they grow.\",\n",
        "    \"Cats have a specialized collarbone that allows them to always land on their feet when falling.\",\n",
        "    \"The Labrador Retriever has been the most popular dog breed in the United States for decades.\",\n",
        "    \"Cats can rotate their ears 180 degrees which makes them great hunters.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "y4V5NFr3_tMj"
      },
      "outputs": [],
      "source": [
        "# # Setup the Azure OpenAI client\n",
        "# from openai import AzureOpenAI\n",
        "# from google.colab import userdata\n",
        "\n",
        "# client = AzureOpenAI(\n",
        "#     azure_endpoint = userdata.get('AZURE_OPENAI_ENDPOINT'),\n",
        "#     api_key=userdata.get('AZURE_OPENAI_API_KEY'),\n",
        "#     api_version=\"2024-06-01\" )\n",
        "\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2024-02-01\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEIb_oKSWbk"
      },
      "source": [
        "For creating a vector database, we need an embedding that transforms each piece of text into an embedding, e.g. a numerical representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lFyUqvLGSXNe"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_MODEL = \"text-embedding-ada-002\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v2GFDpYStxZ"
      },
      "source": [
        "The simple_vector_database will be a list of tuples containing the original text and the embedding of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "B_109crxATOe"
      },
      "outputs": [],
      "source": [
        "# Create a vector DB\n",
        "simple_vector_db = []\n",
        "\n",
        "def get_embedding(chunk):\n",
        "    response = client.embeddings.create(\n",
        "        input=[chunk],\n",
        "        model=EMBEDDING_MODEL\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def add_chunk_to_database(chunk):\n",
        "    embedding = get_embedding(chunk)\n",
        "    simple_vector_db.append((chunk, embedding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RKk32JLMAqjs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added chunk 1/11 to the database\n",
            "Added chunk 2/11 to the database\n",
            "Added chunk 3/11 to the database\n",
            "Added chunk 4/11 to the database\n",
            "Added chunk 5/11 to the database\n",
            "Added chunk 6/11 to the database\n",
            "Added chunk 7/11 to the database\n",
            "Added chunk 8/11 to the database\n",
            "Added chunk 9/11 to the database\n",
            "Added chunk 10/11 to the database\n",
            "Added chunk 11/11 to the database\n"
          ]
        }
      ],
      "source": [
        "# Populate the database\n",
        "for i, chunk in enumerate(dataset):\n",
        "    add_chunk_to_database(chunk)\n",
        "    print(f'Added chunk {i+1}/{len(dataset)} to the database')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK0cifxDTiAd"
      },
      "source": [
        "The embedding created has the dimension of 1536, i.e. 1536 numbers representing the original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fheAbdImTXEB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(simple_vector_db[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDvzVMBLTrwD"
      },
      "source": [
        "Now that we have a vector database, we need to have a way to retrieve from it.\n",
        "\n",
        "We will use the embeddings to retrieve texts that are numerically similar to an input query. For this we need a function that calculates the cosine similarity between two texts. The retrieve function then calculates the embedding of the input query and the embeddings of the texts in the vector db, returning the top_n most similar texts from the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sJi4dzVjAKXd"
      },
      "outputs": [],
      "source": [
        "# Define similarity Retriever\n",
        "def cosine_similarity(a, b):\n",
        "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
        "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
        "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "def retrieve(query, top_n=3):\n",
        "    query_embedding = get_embedding(query)\n",
        "    similarities = []\n",
        "    for chunk, embedding in simple_vector_db:\n",
        "        similarity = cosine_similarity(query_embedding, embedding)\n",
        "        similarities.append((chunk, similarity))\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0sznoSNUhgo"
      },
      "source": [
        "Let's try our vector db retriever:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "mhu2slcUUDrF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Cats can rotate their ears 180 degrees to better locate sounds.',\n",
              "  0.9183710617759656),\n",
              " ('Cats can rotate their ears 180 degrees which makes them great hunters.',\n",
              "  0.8980675589993741),\n",
              " ('Cats have a specialized collarbone that allows them to always land on their feet when falling.',\n",
              "  0.8194481567208423)]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieve('How far can cats rotate their ears?')\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0xlWlxIU-aq"
      },
      "source": [
        "The last step to a RAG chatbot is to pass the retrieved information along with the original query to a LLM that generates an answer based on the information it receives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "fdccfCmLWRZo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved knowledge:\n",
            " - (similarity: 0.92) Cats can rotate their ears 180 degrees to better locate sounds.\n",
            " - (similarity: 0.90) Cats can rotate their ears 180 degrees which makes them great hunters.\n",
            " - (similarity: 0.83) Cats have a specialized collarbone that allows them to always land on their feet when falling.\n",
            "Chatbot response:\n",
            "Cats can rotate their ears 180 degrees to better locate sounds.\n"
          ]
        }
      ],
      "source": [
        "# Chatbot\n",
        "\n",
        "GENERATION_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "input_query = 'How far can cats rotate their ears? '\n",
        "\n",
        "retrieved_knowledge = retrieve(input_query)\n",
        "\n",
        "print('Retrieved knowledge:')\n",
        "for chunk, similarity in retrieved_knowledge:\n",
        "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
        "\n",
        "instruction_prompt = f\"\"\"You are a helpful chatbot.\n",
        "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
        "{chr(10).join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=GENERATION_MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": instruction_prompt},\n",
        "        {\"role\": \"user\", \"content\": input_query},\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print the chatbot response\n",
        "print('Chatbot response:')\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRGIXOfvWn7Q"
      },
      "source": [
        "## Assignment:\n",
        "Try out what happens, if you ask the chatbot something that is not in the cats & dogs datase, e.g. how to make a full English breakfast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "MIySzpBuzyEM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved knowledge:\n",
            " - (similarity: 0.72) A group of kittens is called a 'kindle,' while a group of adult cats is called a 'clowder.'\n",
            " - (similarity: 0.71) Dalmatians are born completely white and develop their spots as they grow.\n",
            " - (similarity: 0.70) Cats spend approximately 70% of their lives sleeping.\n",
            "Chatbot response:\n"
          ]
        }
      ],
      "source": [
        "# Chatbot\n",
        "\n",
        "GENERATION_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "input_query = \"How to make a full English breakfast?\"\n",
        "\n",
        "retrieved_knowledge = retrieve(input_query)\n",
        "\n",
        "print(\"Retrieved knowledge:\")\n",
        "for chunk, similarity in retrieved_knowledge:\n",
        "    print(f\" - (similarity: {similarity:.2f}) {chunk}\")\n",
        "\n",
        "instruction_prompt = f\"\"\"You are a helpful chatbot.\n",
        "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
        "{chr(10).join([f\" - {chunk}\" for chunk, similarity in retrieved_knowledge])}\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=GENERATION_MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": instruction_prompt},\n",
        "        {\"role\": \"user\", \"content\": input_query},\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Print the chatbot response\n",
        "print(\"Chatbot response:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7WqOMXDajJ"
      },
      "source": [
        "## Example: Populate a ChromaDB vector database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yB0HkmMV7ps"
      },
      "source": [
        "Instead of just having a list of tuples, lets now use the cats&dogs dataset to pupulate a vector db with Chroma DB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ij0cu51XKDgc"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_chroma import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Vtd7Bd5DKGhO"
      },
      "outputs": [],
      "source": [
        "# Initialize the embedding functions and ChromaDB\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    model=EMBEDDING_MODEL,\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2024-06-01\",\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        ")\n",
        "\n",
        "# client = AzureOpenAI(\n",
        "#     azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "#     api_key=\n",
        "#     api_version=\"2024-02-01\",\n",
        "# )\n",
        "vector_db = Chroma(\n",
        "    collection_name=\"cats-and-dogs\",\n",
        "    embedding_function=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TV-ITFIfDhxM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['6cc6b3ab-b4ce-490d-8d2c-2d13f2dd63ea',\n",
              " '03dd3db8-a87d-4ac6-9370-eee9f26e56a4',\n",
              " '98c1410e-cd16-4d6f-836e-14f939eeb978',\n",
              " '96931f4f-c14d-40cb-b2ee-2099bd7a3ee3',\n",
              " 'd9534f66-ca71-43b7-aa17-18ef76d9029b',\n",
              " 'aae2dc53-cb4d-4119-96aa-e3d237a67b92',\n",
              " '6473bd8d-d570-42f5-ad86-8562f078cf36',\n",
              " 'e9b8e6e2-89a6-4cce-b71a-b253cddd3014',\n",
              " '2db85562-aa8f-4ec2-81b1-416d9d2d88f4',\n",
              " '4268dbb8-6373-4cff-8e07-d852fe1f79ba',\n",
              " '6011bc3f-c29a-4c76-8daa-5db751ef5787']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add texts to the vectorDB\n",
        "vector_db.add_texts(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "53szD0o8KJ3E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Cats can rotate their ears 180 degrees to better locate sounds.\n",
            "* Cats can rotate their ears 180 degrees which makes them great hunters.\n"
          ]
        }
      ],
      "source": [
        "results = vector_db.similarity_search(\n",
        "    \"How far can cats rotate their ears\",\n",
        "    k=2,\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2TpCFHfW9R5"
      },
      "source": [
        "## Assignment:\n",
        "\n",
        "Try out what happens if you perform a similarity search on the input queries \"How far can dogs rotate their ears?\". Why do you think that is? Could this be a problem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "odUpQyIQYzJo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Cats can rotate their ears 180 degrees to better locate sounds.\n",
            "* Cats can rotate their ears 180 degrees which makes them great hunters.\n"
          ]
        }
      ],
      "source": [
        "results2 = vector_db.similarity_search(\n",
        "    \"How far can dogs rotate their ears\",\n",
        "    k=2,\n",
        ")\n",
        "for res in results2:\n",
        "    print(f\"* {res.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE9zqLZGEjDw"
      },
      "source": [
        "## Example: Metadata filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJUoYRQnXcNM"
      },
      "source": [
        "One way to solve the problem observed above is to introduce metadata into the vector DB. In this case, it would be a label indicating be whether the text relates to cats or dogs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "0yCCuKwwNpiQ"
      },
      "outputs": [],
      "source": [
        "# Create a new dataset\n",
        "vector_db_metadata = Chroma(\n",
        "    collection_name=\"cats-and-dogs-metadata\",\n",
        "    embedding_function=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "yn2V4wcdN-jC"
      },
      "outputs": [],
      "source": [
        "metadata = [{\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"}, {\"Animal\": \"Dog\"},\n",
        "            {\"Animal\": \"Cat\"},]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuKCDtQdX0Ke"
      },
      "source": [
        "You can pass metadata objects as dictionaries when you add data to the vector DB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ck8yhMYWN0E8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x20b54b8b200>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db_metadata.from_texts(\n",
        "    dataset,\n",
        "    collection_name=\"cats-and-dogs-metadata\",\n",
        "    metadatas = metadata,\n",
        "    embedding = embeddings\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbOGr-0HX9-V"
      },
      "source": [
        "In a vector DB with metadata you can perform similarity search with previous filtering, i.e. for a question concerning docs you only look at entries that have that attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "2KA667KQNnAL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Dogs have a sense of smell that's 10,000 to 100,000 times more sensitive than humans.\n",
            "* Dogs have a sense of smell that's 10,000 to 100,000 times more sensitive than humans.\n"
          ]
        }
      ],
      "source": [
        "results = vector_db_metadata.similarity_search(\n",
        "    \"How far can dogs rotate their ears?\",\n",
        "    k=2,\n",
        "    filter={\"Animal\": \"Dog\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLGNPVIjD4l5"
      },
      "source": [
        "## Example: MMR retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg9HTcyuY30U"
      },
      "source": [
        "Max mariginal relevance is a different retrieval method. Execute the code cell below and obeserve how the result changes in comparison to the standard similiarity search. Why do you think that is?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "olsEDRyxMqb-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='6cc6b3ab-b4ce-490d-8d2c-2d13f2dd63ea', metadata={}, page_content='Cats can rotate their ears 180 degrees to better locate sounds.'),\n",
              " Document(id='2db85562-aa8f-4ec2-81b1-416d9d2d88f4', metadata={}, page_content='Cats have a specialized collarbone that allows them to always land on their feet when falling.')]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.max_marginal_relevance_search(\n",
        "    query = \"How far can cats rotate their ears\",\n",
        "    k = 2,\n",
        "    fetch_k = 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ1agLHjEbyJ"
      },
      "source": [
        "## Example: Similarity Threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqS3o2mGeVr1"
      },
      "source": [
        "You can also set a similarity threshold, meaning that results are only retrieved from the db if their similiarity is above a defined threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "REVP912PcV33"
      },
      "outputs": [],
      "source": [
        "retriever = vector_db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.4}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-nQ2u3YLcodd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='6473bd8d-d570-42f5-ad86-8562f078cf36', metadata={}, page_content=\"A group of kittens is called a 'kindle,' while a group of adult cats is called a 'clowder.'\"),\n",
              " Document(id='2db85562-aa8f-4ec2-81b1-416d9d2d88f4', metadata={}, page_content='Cats have a specialized collarbone that allows them to always land on their feet when falling.'),\n",
              " Document(id='d9534f66-ca71-43b7-aa17-18ef76d9029b', metadata={}, page_content='Domestic cats are descendants of African wildcats, which were first domesticated about 9,000 years ago.'),\n",
              " Document(id='98c1410e-cd16-4d6f-836e-14f939eeb978', metadata={}, page_content='Cats spend approximately 70% of their lives sleeping.')]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"How do you call a group of kittens?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "29-NuQtXd4Ei"
      },
      "outputs": [],
      "source": [
        "retriever = vector_db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "aKNuh3Icd7tE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='6473bd8d-d570-42f5-ad86-8562f078cf36', metadata={}, page_content=\"A group of kittens is called a 'kindle,' while a group of adult cats is called a 'clowder.'\")]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"How do you call a group of kittens?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "8K-4EhbFeJul"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No relevant docs were retrieved using the relevance score threshold 0.8\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"How do I make a group of pancakes?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7m6RnfVEfAO"
      },
      "source": [
        "## Example: Ensemble Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuyvjBlnfGJx"
      },
      "source": [
        "You do not have to use a single retriever, but can combine different retrieval methods with an EnsembleRetriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "QPhulLVdQE5g"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "EYXVTxLPOT9m"
      },
      "outputs": [],
      "source": [
        "semantic_retriever= vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "\n",
        "keyword_retriever = BM25Retriever.from_texts(dataset)\n",
        "keyword_retriever.k =  2\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[semantic_retriever,\n",
        "                                                   keyword_retriever],\n",
        "                                       weights=[0.5, 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "WAVUULKUe4Sb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='6473bd8d-d570-42f5-ad86-8562f078cf36', metadata={}, page_content=\"A group of kittens is called a 'kindle,' while a group of adult cats is called a 'clowder.'\"),\n",
              " Document(id='2db85562-aa8f-4ec2-81b1-416d9d2d88f4', metadata={}, page_content='Cats have a specialized collarbone that allows them to always land on their feet when falling.')]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retriever.invoke(\"How do you call a group of kittens?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "pF0_SevEe_au"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content=\"A group of kittens is called a 'kindle,' while a group of adult cats is called a 'clowder.'\"),\n",
              " Document(metadata={}, page_content=\"Dogs have a sense of smell that's 10,000 to 100,000 times more sensitive than humans.\")]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keyword_retriever.invoke(\"How do you call a group of kittens?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "n9tR4cWVexQO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='6473bd8d-d570-42f5-ad86-8562f078cf36', metadata={}, page_content=\"A group of kittens is called a 'kindle,' while a group of adult cats is called a 'clowder.'\"),\n",
              " Document(id='2db85562-aa8f-4ec2-81b1-416d9d2d88f4', metadata={}, page_content='Cats have a specialized collarbone that allows them to always land on their feet when falling.'),\n",
              " Document(metadata={}, page_content=\"Dogs have a sense of smell that's 10,000 to 100,000 times more sensitive than humans.\")]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retriever.invoke(\"How do you call a group of kittens?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm1WldMLC2eW"
      },
      "source": [
        "## Example: Build a RAG chatbot with langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlkOmr0nZQTU"
      },
      "source": [
        "Now let's rebuild the simple, from scratch cats and dogs retriever from above using langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "xHnBzqTQaHRH"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "DQUJyubqJpXp"
      },
      "outputs": [],
      "source": [
        "# Initialize the model you want to use\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2023-06-01-preview\",\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        ")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "eg6e9oZFI10E"
      },
      "outputs": [],
      "source": [
        "# Define a retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Zm4bRBILZ5qw"
      },
      "outputs": [],
      "source": [
        "# Define a prompt for the generation\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know..\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjHXt3tZykK"
      },
      "source": [
        "Here we will define the chatbot as a chain, that first uses the retriever and a formatting function that prepares the output in a for the LLM, than populates the prompt with the results, calls the LLM and returns the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "qHOgR6ywCtRK"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUXJmBHzaU3b"
      },
      "source": [
        "Finally, invoking the chain will give you the response of the chatbot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "531LaRlqaTlS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Cats can rotate their ears 180 degrees to better locate sounds.'"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke(\"How far can cats rotate their ears?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3lvdZgEC7aS"
      },
      "source": [
        "## Example: Add website text as corpus to the RAG chabot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvLn6hVHcIe1"
      },
      "source": [
        "So far, we used just a few sentences on cats and dogs as context. Vector databases will be typically containing many more and much larger documents. So, let's build a vector db that contains wikipedia articles on some movies. Langchain offeres a retriever class to get articles from Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "gpuylVUQiS0K"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "\n",
        "retriever = WikipediaRetriever(\n",
        "    top_k_results = 1,\n",
        "    lang = 'en',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "xyWLVOvUipMR"
      },
      "outputs": [],
      "source": [
        "movie_docs =[]\n",
        "movies = [\"Inception\", \"Django Unchained\", \"Shutter Island\", \"The Dark Knight\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "T52Ca5ldgJZg"
      },
      "outputs": [],
      "source": [
        "for movie in movies:\n",
        "  movie_docs += retriever.invoke(movie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_2Jluj7iAnf"
      },
      "source": [
        "The retriever returns a document object, which is used by langchain to process and populate databases. It contains metadata (in this case title, source link and a summary) and the actual text retrieved as page content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "K1qu6-Roj1ZZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'title': 'Inception', 'summary': 'Inception is a 2010  science fiction  action  heist film written and directed by Christopher Nolan, who also produced it with Emma Thomas, his wife. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets. He is offered a chance to have his criminal history erased as payment for the implantation of another person\\'s idea into a target\\'s subconscious. The ensemble cast includes Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Cillian Murphy, Tom Berenger, Dileep Rao, and Michael Caine.\\nAfter the 2002 completion of Insomnia, Nolan presented to Warner Bros. a written 80-page treatment for a horror film envisioning \"dream stealers,\" based on lucid dreaming. Deciding he needed more experience before tackling a production of this magnitude and complexity, Nolan shelved the project and instead worked on 2005\\'s Batman Begins, 2006\\'s The Prestige, and 2008\\'s The Dark Knight. The treatment was revised over six months and was purchased by Warner in February 2009. Inception was filmed in six countries, beginning in Tokyo on June 19 and ending in Canada on November 22. Its official budget was $160 million, split between Warner Bros. and Legendary. Nolan\\'s reputation and success with The Dark Knight helped secure the film\\'s US$100 million in advertising expenditure.\\nInception\\'s premiere was held in London on July 8, 2010; it was released in both conventional and IMAX theaters beginning on July 16, 2010. Inception grossed over $839 million worldwide, becoming the fourth-highest-grossing film of 2010. Considered one of the best films of the 2010s and the 21st century, Inception, among its numerous accolades, won four Oscars (Best Cinematography, Best Sound Editing, Best Sound Mixing, Best Visual Effects) and was nominated for four more (Best Picture, Best Original Screenplay, Best Art Direction, Best Original Score) at the 83rd Academy Awards.', 'source': 'https://en.wikipedia.org/wiki/Inception'}, page_content='Inception is a 2010  science fiction  action  heist film written and directed by Christopher Nolan, who also produced it with Emma Thomas, his wife. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets. He is offered a chance to have his criminal history erased as payment for the implantation of another person\\'s idea into a target\\'s subconscious. The ensemble cast includes Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Cillian Murphy, Tom Berenger, Dileep Rao, and Michael Caine.\\nAfter the 2002 completion of Insomnia, Nolan presented to Warner Bros. a written 80-page treatment for a horror film envisioning \"dream stealers,\" based on lucid dreaming. Deciding he needed more experience before tackling a production of this magnitude and complexity, Nolan shelved the project and instead worked on 2005\\'s Batman Begins, 2006\\'s The Prestige, and 2008\\'s The Dark Knight. The treatment was revised over six months and was purchased by Warner in February 2009. Inception was filmed in six countries, beginning in Tokyo on June 19 and ending in Canada on November 22. Its official budget was $160 million, split between Warner Bros. and Legendary. Nolan\\'s reputation and success with The Dark Knight helped secure the film\\'s US$100 million in advertising expenditure.\\nInception\\'s premiere was held in London on July 8, 2010; it was released in both conventional and IMAX theaters beginning on July 16, 2010. Inception grossed over $839 million worldwide, becoming the fourth-highest-grossing film of 2010. Considered one of the best films of the 2010s and the 21st century, Inception, among its numerous accolades, won four Oscars (Best Cinematography, Best Sound Editing, Best Sound Mixing, Best Visual Effects) and was nominated for four more (Best Picture, Best Original Screenplay, Best Art Direction, Best Original Score) at the 83rd Academy Awards.\\n\\n\\n== Plot ==\\nDom Cobb and Arthur are \"extractors\" who perform corporate espionage using experimental dream-sharing technology to infiltrate their targets\\' subconscious and extract information. Their latest target, Saito, is impressed with Cobb\\'s ability to layer multiple dreams within each other. He offers to hire Cobb for the ostensibly impossible job of implanting an idea into a person\\'s subconscious; performing \"inception\" on Robert Fischer, the son of Saito\\'s competitor Maurice Fischer, with the idea to dissolve his father\\'s company. In return, Saito promises to clear Cobb\\'s criminal status, allowing him to return home to his children.\\nCobb accepts the offer and assembles his team: a forger named Eames, a chemist named Yusuf, and a college student named Ariadne. Ariadne is tasked with designing the dream\\'s architecture, something Cobb himself cannot do for fear of being sabotaged by his mind\\'s projection of his late wife, Mal. Maurice Fischer dies, and the team sedates Robert Fischer into a three-layer shared dream on an airplane to America bought by Saito. Time on each layer runs slower than the layer above, with one member staying behind on each to perform a music-synchronized \"kick\" (using the French song \"Non, je ne regrette rien\") to awaken dreamers on all three levels simultaneously.\\nThe team abducts Robert in a city on the first level, but unknown to any team member, his subconscious projections, trained to anticipate such a scenario, attack them. After Saito is wounded, Cobb reveals that while dying in the dream would usually awaken dreamers, Yusuf\\'s sedatives will instead send them into \"Limbo\": a world of infinite subconscious. Eames impersonates Robert\\'s godfather, Peter Browning, to introduce the idea of an alternate will to dissolve the company.\\nCobb explains to Ariadne that he and Mal entered Limbo while experimenting with dream-sharing, experiencing fifty years in one night due to the time dilation with reality. After waking up, Mal still believed she was dreaming. Attempting to \"wake u')"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kbF3eNGijzc"
      },
      "source": [
        "Longer documents can be split up into smaller parts with the RecursiveCharacterTextSplitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "7zQ9kzDcJWTY"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
        "\n",
        "movie_docs_split = text_splitter.split_documents(movie_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "ipNE2pyGi9PM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inception\n",
            "Django Unchained\n",
            "Shutter Island (film)\n",
            "The Dark Knight\n"
          ]
        }
      ],
      "source": [
        "for movie in movie_docs:\n",
        "  print(movie.metadata['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "8iLG1wJTBiZA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(movie_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "xPiSpJrjjYoH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(movie_docs_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-laYlK5ClOD8"
      },
      "source": [
        "Now lets create a vector db from the split documents and define a movie RAG chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "d_3zAqZ7JePr"
      },
      "outputs": [],
      "source": [
        "movie_vector_db = Chroma.from_documents(documents=movie_docs_split, embedding=embeddings, persist_directory=\"persist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "_wcXqGAaBoIi"
      },
      "outputs": [],
      "source": [
        "movie_retriever = movie_vector_db.as_retriever(search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "JaiDwOLLkvBp"
      },
      "outputs": [],
      "source": [
        "# Define a prompt for the generation\n",
        "movie_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an assistant for question on famous movies. Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know. Do not answer any question that are not related to movies.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "-uk1TJ4RkB9M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='abcf179b-9e0f-4540-9e85-fa3cbac8df20', metadata={'source': 'https://en.wikipedia.org/wiki/Inception', 'summary': 'Inception is a 2010  science fiction  action  heist film written and directed by Christopher Nolan, who also produced it with Emma Thomas, his wife. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets. He is offered a chance to have his criminal history erased as payment for the implantation of another person\\'s idea into a target\\'s subconscious. The ensemble cast includes Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Cillian Murphy, Tom Berenger, Dileep Rao, and Michael Caine.\\nAfter the 2002 completion of Insomnia, Nolan presented to Warner Bros. a written 80-page treatment for a horror film envisioning \"dream stealers,\" based on lucid dreaming. Deciding he needed more experience before tackling a production of this magnitude and complexity, Nolan shelved the project and instead worked on 2005\\'s Batman Begins, 2006\\'s The Prestige, and 2008\\'s The Dark Knight. The treatment was revised over six months and was purchased by Warner in February 2009. Inception was filmed in six countries, beginning in Tokyo on June 19 and ending in Canada on November 22. Its official budget was $160 million, split between Warner Bros. and Legendary. Nolan\\'s reputation and success with The Dark Knight helped secure the film\\'s US$100 million in advertising expenditure.\\nInception\\'s premiere was held in London on July 8, 2010; it was released in both conventional and IMAX theaters beginning on July 16, 2010. Inception grossed over $839 million worldwide, becoming the fourth-highest-grossing film of 2010. Considered one of the best films of the 2010s and the 21st century, Inception, among its numerous accolades, won four Oscars (Best Cinematography, Best Sound Editing, Best Sound Mixing, Best Visual Effects) and was nominated for four more (Best Picture, Best Original Screenplay, Best Art Direction, Best Original Score) at the 83rd Academy Awards.', 'title': 'Inception'}, page_content=\"Inception is a 2010  science fiction  action  heist film written and directed by Christopher Nolan, who also produced it with Emma Thomas, his wife. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets. He is offered a chance to have his criminal history erased as payment for the implantation of another person's idea into a target's subconscious. The ensemble cast includes Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Cillian Murphy, Tom Berenger, Dileep Rao, and Michael Caine.\"),\n",
              " Document(id='be513c23-8451-4c93-a580-4b1dbf84b3f3', metadata={'source': 'https://en.wikipedia.org/wiki/Inception', 'title': 'Inception', 'summary': 'Inception is a 2010  science fiction  action  heist film written and directed by Christopher Nolan, who also produced it with Emma Thomas, his wife. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets. He is offered a chance to have his criminal history erased as payment for the implantation of another person\\'s idea into a target\\'s subconscious. The ensemble cast includes Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Cillian Murphy, Tom Berenger, Dileep Rao, and Michael Caine.\\nAfter the 2002 completion of Insomnia, Nolan presented to Warner Bros. a written 80-page treatment for a horror film envisioning \"dream stealers,\" based on lucid dreaming. Deciding he needed more experience before tackling a production of this magnitude and complexity, Nolan shelved the project and instead worked on 2005\\'s Batman Begins, 2006\\'s The Prestige, and 2008\\'s The Dark Knight. The treatment was revised over six months and was purchased by Warner in February 2009. Inception was filmed in six countries, beginning in Tokyo on June 19 and ending in Canada on November 22. Its official budget was $160 million, split between Warner Bros. and Legendary. Nolan\\'s reputation and success with The Dark Knight helped secure the film\\'s US$100 million in advertising expenditure.\\nInception\\'s premiere was held in London on July 8, 2010; it was released in both conventional and IMAX theaters beginning on July 16, 2010. Inception grossed over $839 million worldwide, becoming the fourth-highest-grossing film of 2010. Considered one of the best films of the 2010s and the 21st century, Inception, among its numerous accolades, won four Oscars (Best Cinematography, Best Sound Editing, Best Sound Mixing, Best Visual Effects) and was nominated for four more (Best Picture, Best Original Screenplay, Best Art Direction, Best Original Score) at the 83rd Academy Awards.'}, page_content=\"Inception's premiere was held in London on July 8, 2010; it was released in both conventional and IMAX theaters beginning on July 16, 2010. Inception grossed over $839 million worldwide, becoming the fourth-highest-grossing film of 2010. Considered one of the best films of the 2010s and the 21st century, Inception, among its numerous accolades, won four Oscars (Best Cinematography, Best Sound Editing, Best Sound Mixing, Best Visual Effects) and was nominated for four more (Best Picture, Best Original Screenplay, Best Art Direction, Best Original Score) at the 83rd Academy Awards.\"),\n",
              " Document(id='bc64e1ed-c944-4964-b647-c3aac2061172', metadata={'source': 'https://en.wikipedia.org/wiki/Inception', 'summary': 'Inception is a 2010  science fiction  action  heist film written and directed by Christopher Nolan, who also produced it with Emma Thomas, his wife. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets. He is offered a chance to have his criminal history erased as payment for the implantation of another person\\'s idea into a target\\'s subconscious. The ensemble cast includes Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Cillian Murphy, Tom Berenger, Dileep Rao, and Michael Caine.\\nAfter the 2002 completion of Insomnia, Nolan presented to Warner Bros. a written 80-page treatment for a horror film envisioning \"dream stealers,\" based on lucid dreaming. Deciding he needed more experience before tackling a production of this magnitude and complexity, Nolan shelved the project and instead worked on 2005\\'s Batman Begins, 2006\\'s The Prestige, and 2008\\'s The Dark Knight. The treatment was revised over six months and was purchased by Warner in February 2009. Inception was filmed in six countries, beginning in Tokyo on June 19 and ending in Canada on November 22. Its official budget was $160 million, split between Warner Bros. and Legendary. Nolan\\'s reputation and success with The Dark Knight helped secure the film\\'s US$100 million in advertising expenditure.\\nInception\\'s premiere was held in London on July 8, 2010; it was released in both conventional and IMAX theaters beginning on July 16, 2010. Inception grossed over $839 million worldwide, becoming the fourth-highest-grossing film of 2010. Considered one of the best films of the 2010s and the 21st century, Inception, among its numerous accolades, won four Oscars (Best Cinematography, Best Sound Editing, Best Sound Mixing, Best Visual Effects) and was nominated for four more (Best Picture, Best Original Screenplay, Best Art Direction, Best Original Score) at the 83rd Academy Awards.', 'title': 'Inception'}, page_content='After the 2002 completion of Insomnia, Nolan presented to Warner Bros. a written 80-page treatment for a horror film envisioning \"dream stealers,\" based on lucid dreaming. Deciding he needed more experience before tackling a production of this magnitude and complexity, Nolan shelved the project and instead worked on 2005\\'s Batman Begins, 2006\\'s The Prestige, and 2008\\'s The Dark Knight. The treatment was revised over six months and was purchased by Warner in February 2009. Inception was filmed in six countries, beginning in Tokyo on June 19 and ending in Canada on November 22. Its official budget was $160 million, split between Warner Bros. and Legendary. Nolan\\'s reputation and success with The Dark Knight helped secure the film\\'s US$100 million in advertising expenditure.'),\n",
              " Document(id='2b3f5e57-27bc-45bd-89ea-6991c830d5b1', metadata={'source': 'https://en.wikipedia.org/wiki/Inception', 'title': 'Inception', 'summary': 'Inception is a 2010  science fiction  action  heist film written and directed by Christopher Nolan, who also produced it with Emma Thomas, his wife. The film stars Leonardo DiCaprio as a professional thief who steals information by infiltrating the subconscious of his targets. He is offered a chance to have his criminal history erased as payment for the implantation of another person\\'s idea into a target\\'s subconscious. The ensemble cast includes Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Cillian Murphy, Tom Berenger, Dileep Rao, and Michael Caine.\\nAfter the 2002 completion of Insomnia, Nolan presented to Warner Bros. a written 80-page treatment for a horror film envisioning \"dream stealers,\" based on lucid dreaming. Deciding he needed more experience before tackling a production of this magnitude and complexity, Nolan shelved the project and instead worked on 2005\\'s Batman Begins, 2006\\'s The Prestige, and 2008\\'s The Dark Knight. The treatment was revised over six months and was purchased by Warner in February 2009. Inception was filmed in six countries, beginning in Tokyo on June 19 and ending in Canada on November 22. Its official budget was $160 million, split between Warner Bros. and Legendary. Nolan\\'s reputation and success with The Dark Knight helped secure the film\\'s US$100 million in advertising expenditure.\\nInception\\'s premiere was held in London on July 8, 2010; it was released in both conventional and IMAX theaters beginning on July 16, 2010. Inception grossed over $839 million worldwide, becoming the fourth-highest-grossing film of 2010. Considered one of the best films of the 2010s and the 21st century, Inception, among its numerous accolades, won four Oscars (Best Cinematography, Best Sound Editing, Best Sound Mixing, Best Visual Effects) and was nominated for four more (Best Picture, Best Original Screenplay, Best Art Direction, Best Original Score) at the 83rd Academy Awards.'}, page_content='Cobb accepts the offer and assembles his team: a forger named Eames, a chemist named Yusuf, and a college student named Ariadne. Ariadne is tasked with designing the dream\\'s architecture, something Cobb himself cannot do for fear of being sabotaged by his mind\\'s projection of his late wife, Mal. Maurice Fischer dies, and the team sedates Robert Fischer into a three-layer shared dream on an airplane to America bought by Saito. Time on each layer runs slower than the layer above, with one member staying behind on each to perform a music-synchronized \"kick\" (using the French song \"Non, je ne regrette rien\") to awaken dreamers on all three levels simultaneously.')]"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_retriever.invoke(\"Who plays in Inception?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "1f8w5RNVk8pq"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "movie_rag_chain = (\n",
        "    {\"context\": movie_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | movie_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "LNTLJDQ7lDHy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The film Inception stars Leonardo DiCaprio, Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Elliot Page, Tom Hardy, Cillian Murphy, Tom Berenger, Dileep Rao, and Michael Caine.'"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movie_rag_chain.invoke('Who plays in Inception?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh3SnpFxp-tT"
      },
      "source": [
        "## Example: RAG chatbot with memory in chainlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDG4su_aWZe9"
      },
      "source": [
        "Now, let's take the simple chatbot implementation in chainlit and turn it into a RAG chatbot with a vector DB on movies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3rPqWHw5QES"
      },
      "source": [
        "- Make a venv with python -m .venv venv\n",
        "- Activate .venv with .venv\\Scripts\\activate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADgv9AJHBxV7"
      },
      "source": [
        "- Make a .env file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBggOi1bByTa"
      },
      "outputs": [],
      "source": [
        "AZURE_OPENAI_ENDPOINT=<insert endpoint>\n",
        "AZURE_OPENAI_API_KEY=<insert key>\n",
        "QDRANT_ENDPOINT=<insert Qdrant Endpoint>\n",
        "QDRANT_API_KEY=<insert API Key>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwYTzO3pBsTX"
      },
      "source": [
        "- Make a requirements.txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcLm7PjSB-r6"
      },
      "outputs": [],
      "source": [
        "langchain_openai\n",
        "qdrant-client\n",
        "langchain_qdrant\n",
        "langchain_core\n",
        "chainlit\n",
        "openai\n",
        "wikipedia\n",
        "langchain_community\n",
        "python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEJmXfsr5eTs"
      },
      "source": [
        "\n",
        "\n",
        "*  Create the following create_db.py file and execute with python create_db.py to create a qdrant collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx2mE8c05q7k"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import VectorParams, Distance\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "    api_version=\"2024-06-01\",\n",
        "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        ")\n",
        "\n",
        "retriever = WikipediaRetriever(\n",
        "    top_k_results = 1,\n",
        "    lang = 'en',\n",
        ")\n",
        "qdrant_client = QdrantClient(\n",
        "    url=os.getenv('QDRANT_ENDPOINT'),\n",
        "    api_key=os.getenv('QDRANT_API_KEY')\n",
        ")\n",
        "if not qdrant_client.collection_exists(collection_name=\"local_movie_db\"):\n",
        "    qdrant_client.create_collection(\n",
        "        collection_name=\"local_movie_db\",\n",
        "        vectors_config=VectorParams(\n",
        "            size=1536,\n",
        "            distance=Distance.COSINE\n",
        "        )\n",
        "    )\n",
        "qdrant_db = QdrantVectorStore(\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"local_movie_db\",\n",
        "    embedding=embeddings\n",
        ")\n",
        "movie_docs = []\n",
        "movies = [\"Inception\", \"The Return of the King\", \"Shutter Island\", \"The Dark Knight\"]\n",
        "\n",
        "for movie in movies:\n",
        "  movie_docs += retriever.invoke(movie)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
        "\n",
        "movie_docs_split = text_splitter.split_documents(movie_docs)\n",
        "\n",
        "movie_vector_db = qdrant_db.add_documents(documents=movie_docs_split)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quBTyL615t-V"
      },
      "source": [
        "- Create the following rag_chatbot.py file and execute with chainlit run rag_chatbot.py -w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B55DEiTDqCmK"
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "import chainlit as cl\n",
        "import os\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from chainlit.input_widget import Select, Slider\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start_chat():\n",
        "    await cl.ChatSettings([\n",
        "        Select(\n",
        "            id=\"language\",\n",
        "            values=[\"English\", \"Romanian\"],\n",
        "            label=\"Select your preferred language\",\n",
        "            initial_value=\"English\"\n",
        "        ),\n",
        "        Slider(\n",
        "            id=\"Temperature\",\n",
        "            label=\"Temperature\",\n",
        "            initial=0,\n",
        "            min=0,\n",
        "            max=1,\n",
        "            step=0.1\n",
        "        )\n",
        "    ]\n",
        "    ).send()\n",
        "    cl.user_session.set(\"chat_history\", [])\n",
        "    cl.user_session.set(\"client\", AzureOpenAI(\n",
        "        azure_endpoint=os.environ.get('AZURE_OPENAI_ENDPOINT'),\n",
        "        api_key=os.environ.get('AZURE_OPENAI_API_KEY'),\n",
        "        api_version=\"2024-06-01\"))\n",
        "    cl.user_session.set(\"embedding_model\", AzureOpenAIEmbeddings(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "        api_version=\"2024-06-01\",\n",
        "        azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        "    ))\n",
        "    cl.user_session.set(\"qdrant_client\", QdrantClient(\n",
        "            url=os.getenv('QDRANT_ENDPOINT'),\n",
        "            api_key=os.getenv('QDRANT_API_KEY')\n",
        "    ))\n",
        "    cl.user_session.set(\"retriever\", QdrantVectorStore(\n",
        "        collection_name='local_movie_db',\n",
        "        embedding=cl.user_session.get(\"embedding_model\"),\n",
        "        client=cl.user_session.get(\"qdrant_client\")))\n",
        "    if SCENARIO == \"assignment_1\":\n",
        "        qdrant_client = QdrantClient(\n",
        "            url=os.getenv(\"QDRANT_ENDPOINT\"), api_key=os.getenv(\"QDRANT_API_KEY\")\n",
        "        )\n",
        "        cl.user_session.set(\"qdrant_client\", qdrant_client)\n",
        "\n",
        "        retriever = get_retriever(\"English\", embedding_model, qdrant_client)\n",
        "        cl.user_session.set(\"retriever\", retriever)\n",
        "\n",
        "\n",
        "def get_system_prompt(language):\n",
        "    return f\"\"\"\n",
        "    You are a helpful assistant for question on famous movies.\n",
        "    You will formulate all its answers in {language}.\n",
        "    Base you answer only on pieces of information received as context below.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Do not answer any question that are not related to movies.\"\"\"\n",
        "\n",
        "\n",
        "@cl.on_settings_update\n",
        "async def setup_agent(settings):\n",
        "    cl.user_session.set(\"language\", settings[\"language\"])\n",
        "    cl.user_session.set(\"temperature\", settings[\"Temperature\"])\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "@cl.on_message\n",
        "async def message_send(message: cl.Message):\n",
        "    language = cl.user_session.get(\"language\", \"English\")\n",
        "    temperature = cl.user_session.get(\"temperature\", 0)\n",
        "    chat_history = cl.user_session.get(\"chat_history\", [])\n",
        "    retriever = cl.user_session.get(\"retriever\")\n",
        "    client = cl.user_session.get(\"client\")\n",
        "    retrieved_docs = retriever.similarity_search(message.content, k=4)\n",
        "    context = format_docs(retrieved_docs)\n",
        "    system_prompt = get_system_prompt(language)\n",
        "    chat_history.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    chat_history.append({\"role\": \"user\", \"content\": f\"QUESTION: {message.content}\"})\n",
        "    chat_history.append({\"role\": \"system\", \"content\": f\"CONTEXT: {context}\"})\n",
        "    full_response = \"\"\n",
        "    source_elements = []\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=temperature,\n",
        "        stream=True,\n",
        "        messages=[\n",
        "            {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
        "            for m in chat_history\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    msg = cl.Message(content=\"\")\n",
        "    await msg.send()\n",
        "    for chunk in stream:\n",
        "        if not chunk.choices or not chunk.choices[0].delta:\n",
        "            continue\n",
        "\n",
        "        delta = chunk.choices[0].delta.content or \"\"\n",
        "        full_response += delta\n",
        "        await msg.stream_token(delta)\n",
        "    await msg.update()\n",
        "\n",
        "    source_elements.append(cl.Text(content=context, name=\"Context\", display=\"side\"))\n",
        "    msg.content += \"\\n\\nContext\"\n",
        "    msg.elements = source_elements\n",
        "    await msg.update()\n",
        "\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "    cl.user_session.set(\"chat_history\", chat_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5V-oMowU2C9"
      },
      "source": [
        "## Assignment: Vector DB in Romanian and English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SM_spct6gzC"
      },
      "source": [
        "Retrieve wikipedia articles on the movies in Romanian and English. Depending on the language selected, the retriever should only query the articles of the specified language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XSbZkoZ7bap"
      },
      "outputs": [],
      "source": [
        "#Solution TBD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NULNL02kiYR"
      },
      "source": [
        "## Assignment: Standalone question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQh9XGGG6qbb"
      },
      "source": [
        "Implement a reformulation step, that after the first conversation turn reformulates follow-up questions into standalone questions to be used for retrieval and generation. Use only user questions and llm responses for the reformulation step and only send the reformulated question plus retrieved context to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhXeC4LaknX4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm-course-py3.12 (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
